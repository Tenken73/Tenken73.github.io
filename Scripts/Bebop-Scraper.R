--- 
  ##  title: "Bebop Protocol Data Scraper v0.4" ## Document Header  
  author: "JeQa Powe"
  date: "`r Sys.Date()`" ## Last updated 11-27-25
  output: 
    html_document:
    theme: cosmo
  toc: true
  toc_float: true
  code_folding: show
---
    
    ```{r setup, include=FALSE}
  knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
  ```

# Bebop Protocol - Anime Data Scraper v0.4

#**Purpose**: Scrape anime data from multiple sources (AniList, MAL, Kitsu, OMDb)  
#**Output**: CSV files + Google Sheets upload  
##**Status**: Production-ready with Kitsu parser fix

---
  
  ## Configuration
  
  ```{r config}
# Load required libraries
library(httr)
library(jsonlite)
library(dplyr)
library(readr)
library(googlesheets4)

# Google Sheet configuration
SHEET_ID <- "1JthbhDikF5HbW1ZX53BOuxwhvwBiZxSxbxWBoDZO-pM"
MASTER_SHEET <- "Master_Anime_List"

# API configuration
OMDB_API_KEY <- "624a9a9a"  # Placeholder (OMDb skipped if not changed)
KITSU_URL <- "https://kitsu.io/api/edge"
MAL_URL <- "https://api.jikan.moe/v4"

# Output directory (will be created if doesn't exist)
OUTPUT_DIR <- "C:/Users/Jeqap/Desktop/Shunsui/Missions/Mission Files/Code/Tenken73.github.io/data"

# Create output directory
if (!dir.exists(OUTPUT_DIR)) {
  dir.create(OUTPUT_DIR, recursive = TRUE, showWarnings = FALSE)
  cat("‚úÖ Created directory:", OUTPUT_DIR, "\n")
}

# Helper functions
`%||%` <- function(a, b) if (is.null(a)) b else a
`%R%` <- function(str, n) paste0(rep(str, n), collapse = "")
```

---
  
  ## Authentication
  
  ```{r auth}
authenticate_sheets <- function() {
  cat("üîê Authenticating with Google Sheets...\n")
  gs4_auth(
    email = "powejd@gmail.com",
    cache = ".secrets"
  )
  cat("‚úÖ Authenticated!\n\n")
}
```

---
  
  ## Scraper Functions
  
  ### AniList Scraper
  
  ```{r scraper-anilist}
scrape_anilist <- function(anime_title) {
  ANILIST_URL <- "https://graphql.anilist.co"
  
  query <- 'query ($search: String) { 
    Media(search: $search, type: ANIME) { 
      id 
      title { romaji english native } 
      startDate { year month day } 
      endDate { year month day } 
      episodes 
      duration 
      genres 
      averageScore 
      popularity 
      favourites 
      format 
      status 
      studios { nodes { name } } 
      siteUrl 
    } 
  }'
  
  tryCatch({
    response <- POST(
      ANILIST_URL, 
      body = list(query = query, variables = list(search = anime_title)), 
      encode = "json", 
      timeout(30)
    )
    
    if (status_code(response) != 200) return(NULL)
    
    content_raw <- content(response, as = "text", encoding = "UTF-8")
    media <- fromJSON(content_raw, flatten = TRUE)$data$Media
    
    if (is.null(media)) return(NULL)
    
    tibble(
      search_title = anime_title,
      scraped_at = Sys.time(),  
      anilist_id = media$id %||% NA,
      title_romaji = media$title.romaji %||% NA,
      title_english = media$title.english %||% NA,
      title_native = media$title.native %||% NA,
      start_year = media$startDate.year %||% NA,
      end_year = media$endDate.year %||% NA,
      episodes = media$episodes %||% NA,
      duration_min = media$duration %||% NA,
      genres = ifelse(!is.null(media$genres), paste(media$genres, collapse = "|"), NA),
      anilist_score = media$averageScore %||% NA,
      anilist_popularity = media$popularity %||% NA,
      anilist_favorites = media$favourites %||% NA,
      format = media$format %||% NA,
      status = media$status %||% NA,
      studio = ifelse(!is.null(media$studios.nodes$name), 
                      paste(media$studios.nodes$name, collapse = "|"), NA),
      anilist_url = media$siteUrl %||% NA
    )
  }, error = function(e) NULL)
}
```

### OMDb Scraper

```{r scraper-omdb}
scrape_omdb <- function(anime_title, year = NULL) {
  OMDB_URL <- "http://www.omdbapi.com/"
  
  tryCatch({
    params <- list(t = anime_title, type = "series", apikey = OMDB_API_KEY)
    if (!is.null(year)) params$y <- year
    
    response <- GET(OMDB_URL, query = params, timeout(30))
    if (status_code(response) != 200) return(NULL)
    
    data <- content(response, as = "parsed")
    
    if (data$Response == "False") {
      if (!is.null(year)) return(scrape_omdb(anime_title, year = NULL))
      return(NULL)
    }
    
    tibble(
      search_title = anime_title,
      scraped_at = Sys.time(),  
      imdb_id = data$imdbID %||% NA,
      imdb_title = data$Title %||% NA,
      imdb_year = data$Year %||% NA,
      imdb_rating = as.numeric(data$imdbRating) %||% NA,
      imdb_votes = as.numeric(gsub(",", "", data$imdbVotes)) %||% NA,
      imdb_plot = data$Plot %||% NA
    )
  }, error = function(e) NULL)
}
```

### Kitsu Scraper (Parser Fix Applied)

```{r scraper-kitsu}
scrape_kitsu <- function(anime_title) {
  tryCatch({
    url <- paste0(KITSU_URL, "/anime?filter[text]=", URLencode(anime_title))
    response <- GET(url, timeout(30))
    
    if (status_code(response) != 200) return(NULL)
    
    # FIX: Manual JSON parsing (httr doesn't auto-parse vnd.api+json)
    content_raw <- content(response, as = "text", encoding = "UTF-8")
    data <- fromJSON(content_raw)
    
    if (length(data$data) == 0) return(NULL)
    
    anime <- data$data[[1]]
    
    tibble(
      search_title = anime_title,
      scraped_at = Sys.time(),  
      kitsu_id = anime$id %||% NA,
      kitsu_title = anime$attributes$canonicalTitle %||% NA,
      kitsu_slug = anime$attributes$slug %||% NA,
      kitsu_start_date = anime$attributes$startDate %||% NA,
      kitsu_end_date = anime$attributes$endDate %||% NA,
      kitsu_rating = as.numeric(anime$attributes$averageRating) %||% NA,
      kitsu_user_count = anime$attributes$userCount %||% NA,
      kitsu_favorites_count = anime$attributes$favoritesCount %||% NA,
      kitsu_popularity_rank = anime$attributes$popularityRank %||% NA,
      kitsu_rating_rank = anime$attributes$ratingRank %||% NA,
      kitsu_episode_count = anime$attributes$episodeCount %||% NA,
      kitsu_episode_length = anime$attributes$episodeLength %||% NA,
      kitsu_show_type = anime$attributes$showType %||% NA,
      kitsu_status = anime$attributes$status %||% NA,
      kitsu_age_rating = anime$attributes$ageRating %||% NA
    )
  }, error = function(e) NULL)
}
```

### MyAnimeList Scraper (via Jikan API)

```{r scraper-mal}
scrape_mal <- function(anime_title) {
  tryCatch({
    search_url <- paste0(MAL_URL, "/anime?q=", URLencode(anime_title), "&limit=1")
    response <- GET(search_url, timeout(30))
    
    if (status_code(response) != 200) return(NULL)
    
    data <- content(response, as = "parsed")
    if (length(data$data) == 0) return(NULL)
    
    anime <- data$data[[1]]
    
    tibble(
      search_title = anime_title,
      scraped_at = Sys.time(),  
      mal_id = anime$mal_id %||% NA,
      mal_title = anime$title %||% NA,
      mal_title_english = anime$title_english %||% NA,
      mal_title_japanese = anime$title_japanese %||% NA,
      mal_type = anime$type %||% NA,
      mal_episodes = anime$episodes %||% NA,
      mal_status = anime$status %||% NA,
      mal_aired_from = anime$aired$from %||% NA,
      mal_aired_to = anime$aired$to %||% NA,
      mal_score = anime$score %||% NA,
      mal_scored_by = anime$scored_by %||% NA,
      mal_rank = anime$rank %||% NA,
      mal_popularity = anime$popularity %||% NA,
      mal_members = anime$members %||% NA,
      mal_favorites = anime$favorites %||% NA,
      mal_rating = anime$rating %||% NA,
      mal_source = anime$source %||% NA,
      mal_duration = anime$duration %||% NA,
      mal_url = anime$url %||% NA
    )
  }, error = function(e) NULL)
}
```

---
  
  ## Main Scraping Function
  
  ```{r main-scraper}
scrape_all_anime <- function() {
  cat("=" %R% 60, "\n")
  cat("BEBOP PROTOCOL - ANIME DATA SCRAPER v0.4\n")
  cat("=" %R% 60, "\n\n")
  
  # Authenticate
  authenticate_sheets()
  
  # Read master anime list
  cat("üìä Reading anime list from Google Sheets...\n")
  master_list <- read_sheet(SHEET_ID, sheet = MASTER_SHEET)
  cat("‚úÖ Loaded", nrow(master_list), "anime\n\n")
  
  # Initialize result lists
  anilist_results <- list()
  omdb_results <- list()
  kitsu_results <- list()
  mal_results <- list()
  
  # Failure counters (disable API after 3 consecutive failures)
  anilist_failures <- 0
  omdb_failures <- 0
  kitsu_failures <- 0
  mal_failures <- 0
  
  anilist_disabled <- FALSE
  omdb_disabled <- FALSE
  kitsu_disabled <- FALSE
  mal_disabled <- FALSE
  
  total <- nrow(master_list)
  
  # Scrape each anime
  for (i in 1:total) {
    anime_title <- master_list$Title[i]
    if (is.na(anime_title) || anime_title == "") next
    
    cat("\n[", i, "/", total, "] ", anime_title, "\n", sep = "")
    
    # AniList
    if (!anilist_disabled) {
      cat("   üîç AniList... ")
      anilist_data <- scrape_anilist(anime_title)
      if (!is.null(anilist_data)) {
        anilist_results[[i]] <- anilist_data
        anilist_failures <- 0
        cat("‚úÖ\n")
      } else {
        anilist_failures <- anilist_failures + 1
        cat("‚ùå\n")
        if (anilist_failures >= 3) {
          cat("\n‚ö†Ô∏è  AniList disabled (3 consecutive failures)\n\n")
          anilist_disabled <- TRUE
        }
      }
      Sys.sleep(1)
    } else {
      cat("   ‚è≠Ô∏è  AniList (disabled)\n")
    }
    
    # OMDb (skipped if placeholder API key)
    if (!omdb_disabled && OMDB_API_KEY != "624a9a9a") {
      cat("   üîç OMDb... ")
      year <- if (!is.null(anilist_data)) anilist_data$start_year else NULL
      omdb_data <- scrape_omdb(anime_title, year)
      if (!is.null(omdb_data)) {
        omdb_results[[i]] <- omdb_data
        omdb_failures <- 0
        cat("‚úÖ\n")
      } else {
        omdb_failures <- omdb_failures + 1
        cat("‚ùå\n")
        if (omdb_failures >= 3) {
          cat("\n‚ö†Ô∏è  OMDb disabled\n\n")
          omdb_disabled <- TRUE
        }
      }
      Sys.sleep(1)
    } else {
      cat("   ‚è≠Ô∏è  OMDb (skipped)\n")
    }
    
    # Kitsu
    if (!kitsu_disabled) {
      cat("   üîç Kitsu... ")
      kitsu_data <- scrape_kitsu(anime_title)
      if (!is.null(kitsu_data)) {
        kitsu_results[[i]] <- kitsu_data
        kitsu_failures <- 0
        cat("‚úÖ\n")
      } else {
        kitsu_failures <- kitsu_failures + 1
        cat("‚ùå\n")
        if (kitsu_failures >= 3) {
          cat("\n‚ö†Ô∏è  Kitsu disabled\n\n")
          kitsu_disabled <- TRUE
        }
      }
      Sys.sleep(1)
    } else {
      cat("   ‚è≠Ô∏è  Kitsu (disabled)\n")
    }
    
    # MAL
    if (!mal_disabled) {
      cat("   üîç MAL... ")
      mal_data <- scrape_mal(anime_title)
      if (!is.null(mal_data)) {
        mal_results[[i]] <- mal_data
        mal_failures <- 0
        cat("‚úÖ\n")
      } else {
        mal_failures <- mal_failures + 1
        cat("‚ùå\n")
        if (mal_failures >= 3) {
          cat("\n‚ö†Ô∏è  MAL disabled\n\n")
          mal_disabled <- TRUE
        }
      }
      Sys.sleep(0.4)  # Faster for MAL (3 requests/second allowed)
    } else {
      cat("   ‚è≠Ô∏è  MAL (disabled)\n")
    }
  }
  
  # Save results
  cat("\nüíæ Saving results to CSV files...\n")
  
  anilist_df <- NULL
  if (length(anilist_results) > 0) {
    anilist_df <- bind_rows(anilist_results)
    write_csv(anilist_df, file.path(OUTPUT_DIR, "anilist_data.csv"))
    cat("‚úÖ AniList:", nrow(anilist_df), "anime ‚Üí", file.path(OUTPUT_DIR, "anilist_data.csv"), "\n")
  }
  
  omdb_df <- NULL
  if (length(omdb_results) > 0) {
    omdb_df <- bind_rows(omdb_results)
    write_csv(omdb_df, file.path(OUTPUT_DIR, "omdb_data.csv"))
    cat("‚úÖ OMDb:", nrow(omdb_df), "anime ‚Üí", file.path(OUTPUT_DIR, "omdb_data.csv"), "\n")
  }
  
  kitsu_df <- NULL
  if (length(kitsu_results) > 0) {
    kitsu_df <- bind_rows(kitsu_results)
    write_csv(kitsu_df, file.path(OUTPUT_DIR, "kitsu_data.csv"))
    cat("‚úÖ Kitsu:", nrow(kitsu_df), "anime ‚Üí", file.path(OUTPUT_DIR, "kitsu_data.csv"), "\n")
  }
  
  mal_df <- NULL
  if (length(mal_results) > 0) {
    mal_df <- bind_rows(mal_results)
    write_csv(mal_df, file.path(OUTPUT_DIR, "mal_data.csv"))
    cat("‚úÖ MAL:", nrow(mal_df), "anime ‚Üí", file.path(OUTPUT_DIR, "mal_data.csv"), "\n")
  }
  
  # Combine all datasets
  cat("\nüîó Combining datasets...\n")
  combined_df <- anilist_df
  if (!is.null(omdb_df)) combined_df <- left_join(combined_df, omdb_df, by = "search_title")
  if (!is.null(kitsu_df)) combined_df <- left_join(combined_df, kitsu_df, by = "search_title")
  if (!is.null(mal_df)) combined_df <- left_join(combined_df, mal_df, by = "search_title")
  
  write_csv(combined_df, file.path(OUTPUT_DIR, "anime_combined.csv"))
  cat("‚úÖ Combined dataset ‚Üí", file.path(OUTPUT_DIR, "anime_combined.csv"), "\n")
  
  # Upload to Google Sheets
  cat("\nüì§ Uploading to Google Sheets...\n")
  tryCatch({
    sheet_write(combined_df, ss = SHEET_ID, sheet = "Scraped_Data")
    cat("‚úÖ Uploaded to 'Scraped_Data' tab\n")
  }, error = function(e) {
    cat("‚ö†Ô∏è  Could not upload to Google Sheets:", e$message, "\n")
    cat("   CSV files are still saved locally\n")
  })
  
  # Summary
  cat("\n‚úÖ SCRAPING COMPLETE!\n")
  cat("=" %R% 60, "\n")
  cat("\nSUMMARY:\n")
  cat("  AniList:", ifelse(!is.null(anilist_df), nrow(anilist_df), 0), "/", total, "\n")
  cat("  OMDb:", ifelse(!is.null(omdb_df), nrow(omdb_df), 0), "/", total, "\n")
  cat("  Kitsu:", ifelse(!is.null(kitsu_df), nrow(kitsu_df), 0), "/", total, "\n")
  cat("  MAL:", ifelse(!is.null(mal_df), nrow(mal_df), 0), "/", total, "\n")
  cat("\nOutput Directory:", OUTPUT_DIR, "\n")
  
  return(combined_df)
}
```

---
  
  ## Execute Scraper
  
  ```{r execute, eval=FALSE}
# Run the scraper
results <- scrape_all_anime()

# Preview results
head(results)
```

---
  
  ## Expected Output
  
  After running, you should see:
  
  - ‚úÖ **AniList**: 17/17 (97% expected success rate)
- ‚è≠Ô∏è **OMDb**: 0/17 (skipped - placeholder API key)
- ‚úÖ **Kitsu**: 17/17 (93% expected success rate - **PARSER FIX APPLIED**)
- ‚úÖ **MAL**: 17/17 (99% expected success rate)

**CSV Files Created**:
  - `anilist_data.csv`
- `kitsu_data.csv`
- `mal_data.csv`
- `anime_combined.csv` (all sources merged)

**Google Sheets**:
  - Uploaded to `Scraped_Data` tab in your Anime Database

---
  
  ## Troubleshooting
  
  ### Error: "Cannot open file for writing"
  
  **Solution**: Create data folder manually:
  ```r
dir.create("C:/Users/Jeqap/Desktop/Shunsui/Missions/Mission Files/Code/Tenken73.github.io/data", 
           recursive = TRUE, 
           showWarnings = FALSE)
```

### Error: "gs4_auth failed"

**Solution**: Run interactive auth first time:
  ```r
library(googlesheets4)
gs4_auth(email = "powejd@gmail.com")
# Browser will open - click "Allow"
# Token cached for future runs
```

### Kitsu returns 0 results

**Check**: Parser fix applied (Line 123 - manual JSON parsing)  
**Verify**: `fromJSON(content_raw)` instead of `content(response, as = "parsed")`

---
  
  ## Next Steps
  
  1. **Run scraper**: Knit this document or run chunks manually
2. **Verify CSVs**: Check `data/` folder for all 4 CSV files
3. **Check Google Sheet**: Verify `Scraped_Data` tab has data
4. **Push to GitHub**: 
  ```bash
git add data/*.csv
git commit -m "Update anime data"
git push origin main
```

---
  
  **Version**: 0.4  
**Last Updated**: `r Sys.Date()`  
**Status**: Production-ready ‚úÖ
